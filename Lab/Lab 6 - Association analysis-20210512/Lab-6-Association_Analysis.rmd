---
title: "Association Analysis"
output:
  html_document:
    df_print: paged
---
## STAT318/462 Lab 6: 

## Association Analysis using the Apriori algorithm

First, we need to load the arules package:
```{r}
library(arules)
```

We can then read in the .csv file, converting it to a list of transactions:
```{r fig.height=12, fig.width=12}
data = read.transactions(file = "Basket1.txt", format = "basket", sep = ",")
inspect(data)
```

We can now do a range of tasks with this data:

1. we can mine the frequent itemsets, view all the frequent itemsets, and view the five most frequent itemsets with the greatest support using the following code:
```{r}
FreqItemsets = apriori(data, 
                       parameter = list(support = 0.4, target = "frequent itemsets"))
inspect(FreqItemsets)
inspect(sort(FreqItemsets, by = "support")[1:5])
itemFrequencyPlot(data, support = 0.1, cex.names = 0.8)
```

2. We can mine the maximal frequent itemsets, and view all the maximal frequent itemsets using this code:
```{r}
MaxFreqItemsets = apriori(data, 
                          parameter = list(support = 0.3, 
                                           target = "maximally frequent itemsets"))
inspect(MaxFreqItemsets)
summary(MaxFreqItemsets)
```

3. We can mine the closed frequent itemsets, and view all the closed frequent itemsets using this code:
```{r}
ClosedFreqItemsets = apriori(data, 
                             parameter = list(support = 0.4, 
                                              target = "closed frequent itemsets"))
summary(ClosedFreqItemsets)
```

4. We can mine the association rules, view all the rules, and view the five rules which have the highest confidence as follows:
```{r}
rules = apriori(data,
                parameter = list(minlen = 2, support = 0.4,
                                 confidence = 0.8,target = "rules"))

inspect(rules)

summary(rules)
inspect(sort(rules, by = "confidence")[1:5])
```

5. Finally, we can find particular rules using this code:
```{r}
rules = apriori(data,
                parameter = list(minlen = 2, support = 0.4,
                                 confidence = 0.8, target = "rules"),
                appearance = list(default = "lhs", rhs = "Milk"))
summary(rules)
inspect(sort(rules, by = "confidence"))
```

You can type ?apriori in the console and then click on 'APappearance' to see how to extract more complicated rules.

***

## Other data sets to play with for association analysis:

These are much larger, real-world data sets.

1. Adult Census Income Database
```{r}
data("Adult")
Adult
```

2. Groceries data set contains 1 month (30 days) of real-world point-of-sale transaction data
```{r}
data("Groceries")
Groceries
```

3. The **RealTransactionData_NovDec_2019.xlsx** data set contains 2 months of real-world point-of-sale transaction data.This is a real dataset provided by Thomas Li, and obtained from an industry contact, so it may be used for learning and research work at UC only.

Please see the ReadMe file for data column info. You will need the Transaction ID to match the transactions, and the Product Code to match the product - some data cleaning is needed!

Thomas has cleaned the November 2019 data and stored it in the file **RealTransactionData_Nov_2019.csv**, with two columns: "Transaction ID", and "Product Code". You can read in this transaction data as follows:
```{r}
order_Nov19_trans = read.transactions(
        file = "RealTransactionData_Nov_2019.csv",
        format = "single",
        sep = ",",
        cols = c("Transaction ID", "Product Code"),
        rm.duplicates = T,
        header = T
)
summary(order_Nov19_trans)
```

To find frequent itemsets with a 2% occurance or more, you can use:
```{r}
FreqItemsets = apriori(order_Nov19_trans, 
                       parameter = list(support = 0.02, target = "frequent itemsets"))
inspect(FreqItemsets)
```

You could now find frequent 2 and 3 itemsets, for example, and the association rules.

To repeat this analysis for the December data, you would have to do the data cleaning yourself :)

Then report this back in terms of Product Name rather than Product Code.

***


